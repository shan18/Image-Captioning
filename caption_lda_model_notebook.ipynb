{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Add, Reshape, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.utils import load_coco, load_image, print_progress_bar\n",
    "from models.vgg19 import load_vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the datset\n",
    "data_dir = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, category_id, id_category = load_coco(\n",
    "    os.path.join(data_dir, 'coco_raw.pickle'), 'captions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_captions = train_data  # Load training data\n",
    "val_images, val_captions = val_data  # Load validation data\n",
    "test_images, test_captions = test_data  # Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(id_category)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_images_train = len(train_images)\n",
    "num_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_images_val = len(val_images)\n",
    "num_images_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained feature extractor model\n",
    "feature_model = load_vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type, data_dir):\n",
    "    # Path for the cache-file.\n",
    "    feature_cache_path = os.path.join(\n",
    "        data_dir, 'feature_transfer_values_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    captions_cache_path = os.path.join(\n",
    "        data_dir, 'captions_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    images_cache_path = os.path.join(\n",
    "        data_dir, 'images_{}.pkl'.format(data_type)\n",
    "    )\n",
    "\n",
    "    feature_path_exists = os.path.exists(feature_cache_path)\n",
    "    image_path_exists = os.path.exists(images_cache_path)\n",
    "    caption_path_exists = os.path.exists(captions_cache_path)\n",
    "    if feature_path_exists and image_path_exists and caption_path_exists:\n",
    "        with open(feature_cache_path, mode='rb') as file:\n",
    "            feature_obj = pickle.load(file)\n",
    "        with open(images_cache_path, mode='rb') as file:\n",
    "            images = pickle.load(file)\n",
    "        with open(captions_cache_path, mode='rb') as file:\n",
    "            captions = pickle.load(file)\n",
    "    else:\n",
    "        sys.exit('processed {} data does not exist.'.format(data_type))\n",
    "    \n",
    "    print('{} data loaded from cache-file.'.format(data_type))\n",
    "    return feature_obj, images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topics(data_type, data_dir):\n",
    "    topics_cache_path = os.path.join(\n",
    "        data_dir, 'topics_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    \n",
    "    if os.path.exists(topics_cache_path):\n",
    "        with open(topics_cache_path, mode='rb') as file:\n",
    "            topics = pickle.load(file)\n",
    "    else:\n",
    "        sys.exit('{} does not exist.'.format(topics_cache_path))\n",
    "    \n",
    "    print('{} data topics loaded from cache-file.'.format(data_type))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = os.path.join(data_dir, 'processed_lda_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from cache-file.\n",
      "train data topics loaded from cache-file.\n",
      "CPU times: user 108 ms, sys: 351 ms, total: 459 ms\n",
      "Wall time: 458 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training data\n",
    "feature_transfer_values_train, images_train, captions_train = load_data('train', processed_data_dir)\n",
    "topics_train = load_topics('train', processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val data loaded from cache-file.\n",
      "val data topics loaded from cache-file.\n",
      "CPU times: user 27.9 ms, sys: 34 ms, total: 61.8 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Validation data\n",
    "feature_transfer_values_val, images_val, captions_val = load_data('val', processed_data_dir)\n",
    "topics_val = load_topics('val', processed_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data loaded from cache-file.\n",
      "CPU times: user 9.51 ms, sys: 49.2 ms, total: 58.7 ms\n",
      "Wall time: 57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test data\n",
    "feature_transfer_values_test, images_test, captions_test = load_data('test', processed_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'startseq'\n",
    "mark_end = 'endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_captions(captions_list):\n",
    "    \"\"\" Mark all the captions with the start and the end marker \"\"\"\n",
    "    captions_marked = [\n",
    "        [' '.join([mark_start, caption, mark_end]) for caption in captions] for captions in captions_list\n",
    "    ]\n",
    "    \n",
    "    return captions_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked captions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['startseq A man in a blue shirt is playing tennis. endseq',\n",
       " 'startseq Man running playing tennis with ball in air on court endseq',\n",
       " 'startseq A man playing tennis with his tennis racket and his tennis ball. endseq',\n",
       " 'startseq Male tennis player gets ready to hit the tennis ball. endseq',\n",
       " 'startseq A male tennis player in action on the court. endseq']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train_marked = mark_captions(captions_train)  # training\n",
    "captions_val_marked = mark_captions(captions_val)  # validation\n",
    "print('Marked captions:')\n",
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(captions_list):\n",
    "    \"\"\" Flatten all the captions into a single list \"\"\"\n",
    "    caption_list = [caption\n",
    "                    for caption_list in captions_list\n",
    "                    for caption in caption_list]\n",
    "    \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_flat = flatten(captions_train_marked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 s, sys: 0 ns, total: 1.91 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get integer token for the start marker\n",
    "token_start = tokenizer.word_index[mark_start]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get integer token for the end marker\n",
    "token_end = tokenizer.word_index[mark_end]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8644"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numbers of words in the vocabulary\n",
    "# + 1 is because of reserving padding (i.e. index zero)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of each caption\n",
    "max_tokens = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, topic_transfer_value, feature_transfer_value, caption):\n",
    "    \"\"\" Create sequences of topic_values, feature_values, input sequence and output sequence for an image \"\"\"\n",
    "    topic_values, feature_values = [], []\n",
    "    input_captions, output_captions = [], []\n",
    "    integer_sequence = tokenizer.texts_to_sequences([caption])[0]  # encode the sequence\n",
    "    \n",
    "    for idx in range(1, len(integer_sequence)):\n",
    "        in_seq, out_seq = integer_sequence[:idx], integer_sequence[idx]  # split into input and output pair\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post', truncating='post')[0]  # pad input sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]  # encode output sequence\n",
    "        \n",
    "        # store\n",
    "        topic_values.append(topic_transfer_value)\n",
    "        feature_values.append(feature_transfer_value)\n",
    "        input_captions.append(in_seq)\n",
    "        output_captions.append(out_seq)\n",
    "        \n",
    "    return topic_values, feature_values, input_captions, output_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(topic_transfer_values, feature_transfer_values, captions_list, tokenizer, num_images, batch_size, max_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \n",
    "    It selects the data completely randomly for each\n",
    "    batch, corresponding to sampling of the training-set with\n",
    "    replacement. This means it is possible to sample the same\n",
    "    data multiple times within a single epoch - and it is also\n",
    "    possible that some data is not sampled at all within an epoch.\n",
    "    However, all the data should be unique within a single batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Get a list of random indices for images in the dataset.\n",
    "        indices = np.random.randint(num_images, size=batch_size)\n",
    "        \n",
    "        # For a batch of the randomly chosen images there are\n",
    "        # at least 5 captions describing the contents of the image.\n",
    "        # Select one of those captions at random\n",
    "        topic_values, feature_values = [], []\n",
    "        input_captions, output_captions = [], []\n",
    "        for idx in indices:\n",
    "            topic_value, feature_value, input_caption, output_caption = create_sequences(\n",
    "                tokenizer,\n",
    "                max_length,\n",
    "                topic_transfer_values[idx],\n",
    "                feature_transfer_values[idx],\n",
    "                np.random.choice(captions_list[idx])\n",
    "            )\n",
    "            topic_values.extend(topic_value)\n",
    "            feature_values.extend(feature_value)\n",
    "            input_captions.extend(input_caption)\n",
    "            output_captions.extend(output_caption)\n",
    "\n",
    "        # Dict for the input-data. Because we have\n",
    "        # several inputs, we use a named dict to\n",
    "        # ensure that the data is assigned correctly.\n",
    "        x_data = {\n",
    "            'caption_input': np.array(input_captions),\n",
    "            'topic_input': np.array(topic_values),\n",
    "            'feature_input': np.array(feature_values)\n",
    "        }\n",
    "\n",
    "        # Dict for the output-data.\n",
    "        y_data = {\n",
    "            'caption_output': np.array(output_captions)\n",
    "        }\n",
    "        \n",
    "        yield (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train = batch_generator(  # training\n",
    "    topics_train,\n",
    "    feature_transfer_values_train,\n",
    "    captions_train_marked,\n",
    "    tokenizer,\n",
    "    num_images_train,\n",
    "    batch_size,\n",
    "    max_tokens,\n",
    "    vocab_size\n",
    ")\n",
    "\n",
    "generator_val = batch_generator(  # validation\n",
    "    topics_val,\n",
    "    feature_transfer_values_val,\n",
    "    captions_val_marked,\n",
    "    tokenizer,\n",
    "    num_images_val,\n",
    "    batch_size,\n",
    "    max_tokens,\n",
    "    vocab_size\n",
    ")\n",
    "\n",
    "batch = next(generator_train)\n",
    "batch_x = batch[0]\n",
    "batch_y = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0016129 , 0.03387097, 0.0016129 , 0.01774194, 0.0016129 ,\n",
       "       0.0016129 , 0.05      , 0.0016129 , 0.0016129 , 0.01774194,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.01774194, 0.05      ,\n",
       "       0.0983871 , 0.0016129 , 0.0016129 , 0.0016129 , 0.01774194,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.01774194, 0.08225806, 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.35645161, 0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.01774194, 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.06612903, 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.01774194,\n",
       "       0.0016129 , 0.0016129 , 0.01774194, 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 ,\n",
       "       0.0016129 , 0.0016129 , 0.0016129 , 0.0016129 , 0.01774194])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the topic-transfer-values for the first image in the batch\n",
    "batch_x['topic_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7923771 , 0.        , 0.41744143, ..., 0.        , 0.4082044 ,\n",
       "       0.4943936 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the feature-transfer-values for the first image in the batch\n",
    "batch_x['feature_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the token-sequence for the first image in the batch\n",
    "# This is the input to the decoder-part of the neural network\n",
    "batch_x['caption_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the token-sequence for the output of the decoder\n",
    "# Note how it is the same as the sequence above, except it is shifted one time-step\n",
    "batch_y['caption_output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 8644)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the output\n",
    "batch_y['caption_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steps_per_epoch(captions_list, batch_size):\n",
    "    # Number of captions for each image\n",
    "    num_captions = [len(captions) for captions in captions_list]\n",
    "    \n",
    "    # Total number of captions\n",
    "    total_num_captions = np.sum(num_captions)\n",
    "    \n",
    "    return int(total_num_captions / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train = calculate_steps_per_epoch(captions_train_marked, batch_size)\n",
    "steps_per_epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_val = calculate_steps_per_epoch(captions_val_marked, batch_size)\n",
    "steps_per_epoch_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pre-trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    print('Creating word to vec map...')\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n",
    "    print('Done!')\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word to vec map...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "word_to_vec_map = read_glove_vecs('{}/glove.6B.300d.txt'.format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign embeddings values to custom tokens\n",
    "size = word_to_vec_map['unk'].shape\n",
    "\n",
    "word_to_vec_map[mark_start] = np.random.uniform(low=-1.0, high=1.0, size=size)\n",
    "word_to_vec_map[mark_end] = np.random.uniform(low=-1.0, high=1.0, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index, word_to_vec_map, num_words):\n",
    "    \"\"\" Create a Keras Embedding() layer and load in pre-trained GloVe 300-dimensional vectors\n",
    "        @params:\n",
    "        :word_to_index -- dictionary containing the each word mapped to its index\n",
    "        :word_to_vec_map -- dictionary mapping words to their GloVe vector representation\n",
    "        :num_words -- number of words in the vocabulary\n",
    "        \n",
    "        @return:\n",
    "        :decoder_embedding -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_length = num_words + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    embedding_dimensions = word_to_vec_map['unk'].shape[0]  # define dimensionality of GloVe word vectors (= 300)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocabulary_length, embedding_dimensions))  # initialize with zeros\n",
    "    for word, index in word_to_index.items():\n",
    "        try:\n",
    "            embedding_matrix[index, :] = word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            embedding_matrix[index, :] = word_to_vec_map['unk']\n",
    "    \n",
    "    # we don't want the embeddings to be updated, thus trainable parameter is set to False\n",
    "    decoder_embedding = Embedding(\n",
    "        input_dim=vocabulary_length,\n",
    "        output_dim=embedding_dimensions,\n",
    "        trainable=False,\n",
    "        name='decoder_embedding'\n",
    "    )\n",
    "    decoder_embedding.build((None,))\n",
    "    decoder_embedding.set_weights([embedding_matrix])  # with this the layer is now pretrained\n",
    "    \n",
    "    return decoder_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal state-sizes of LSTMs\n",
    "state_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer to receive the predictions from the feature model\n",
    "feature_input = Input(\n",
    "    shape=K.int_shape(feature_model.output)[1:], name='feature_input'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Dropout Layer\n",
    "feature_net = Dropout(0.5)(feature_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'image_model_output/Relu:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape feature transfer values\n",
    "image_model_output = Dense(state_size, activation='relu', name='image_model_output')(feature_net)\n",
    "image_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model to encode captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This inputs topic-transfer-values to the LSTM\n",
    "topic_input = Input(\n",
    "    shape=topics_train.shape[1:], name='topic_input'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for token-sequences to the decoder\n",
    "# Using 'None' in the shape means that the token-sequences can have arbitrary lengths\n",
    "caption_input = Input(shape=(max_tokens,), name='caption_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding-layer which converts sequences of integer-tokens to sequences of vectors\n",
    "caption_embedding = create_embedding_layer(tokenizer.word_index, word_to_vec_map, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM layer for the input captions\n",
    "caption_lstm = LSTM(state_size, name='caption_lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the topic transfer values to 3D tensor in-order to feed it to the LSTM\n",
    "topic_input_reshaped = Reshape(\n",
    "    target_shape=(topics_train.shape[1:] + (1,))\n",
    ")(topic_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed topic to LSTM\n",
    "_, initial_state_h0, initial_state_c0 = LSTM(\n",
    "    state_size, return_state=True, name='topic_lstm'\n",
    ")(topic_input_reshaped)\n",
    "\n",
    "topic_lstm_states = [initial_state_h0, initial_state_c0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caption_lstm/TensorArrayReadV3:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = caption_input  # Start the decoder-network with its input-layer\n",
    "net = caption_embedding(net)  # Connect the embedding-layer\n",
    "net = Dropout(0.5)(net)\n",
    "caption_model_output = caption_lstm(net, initial_state=topic_lstm_states) # Connect the caption LSTM layer\n",
    "caption_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the image and the caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the outputs of both the models\n",
    "merge_net = Add()([image_model_output, caption_model_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caption_output/Softmax:0' shape=(?, 8644) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_net = Dense(state_size, activation='relu')(merge_net)\n",
    "outputs = Dense(vocab_size, activation='softmax', name='caption_output')(merge_net)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[feature_input, topic_input, caption_input],\n",
    "    outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "topic_input (InputLayer)        (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, 16, 300)      2593500     caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 80, 1)        0           topic_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           feature_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 300)      0           decoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "topic_lstm (LSTM)               [(None, 256), (None, 264192      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "image_model_output (Dense)      (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "caption_lstm (LSTM)             (None, 256)          570368      dropout_1[0][0]                  \n",
      "                                                                 topic_lstm[0][1]                 \n",
      "                                                                 topic_lstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           image_model_output[0][0]         \n",
      "                                                                 caption_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "caption_output (Dense)          (None, 8644)         2221508     dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 6,764,192\n",
      "Trainable params: 4,170,692\n",
      "Non-trainable params: 2,593,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'weights/caption-lda-weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    filepath=path_checkpoint,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(\n",
    "    log_dir='./weights/caption-lda-logs/',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_checkpoint, callback_tensorboard, callback_early_stop, callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights('weights/caption-lda-weights.20-2.25.adam.batch-128.lr-decay.hdf5')\n",
    "    print('Weights loaded.')\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='caption_model_dropout.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 3.4346\n",
      "Epoch 00001: val_loss improved from inf to 2.70062, saving model to weights/caption-lda-weights.01-2.70.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'topic_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "755/755 [==============================] - 418s 553ms/step - loss: 3.4335 - val_loss: 2.7006\n",
      "Epoch 2/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.6123\n",
      "Epoch 00002: val_loss improved from 2.70062 to 2.47448, saving model to weights/caption-lda-weights.02-2.47.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'topic_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "755/755 [==============================] - 414s 549ms/step - loss: 2.6120 - val_loss: 2.4745\n",
      "Epoch 3/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.4293\n",
      "Epoch 00003: val_loss improved from 2.47448 to 2.41824, saving model to weights/caption-lda-weights.03-2.42.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'topic_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "755/755 [==============================] - 415s 550ms/step - loss: 2.4291 - val_loss: 2.4182\n",
      "Epoch 4/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.2924\n",
      "Epoch 00004: val_loss improved from 2.41824 to 2.38259, saving model to weights/caption-lda-weights.04-2.38.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'topic_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "755/755 [==============================] - 414s 548ms/step - loss: 2.2925 - val_loss: 2.3826\n",
      "Epoch 5/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.1952\n",
      "Epoch 00005: val_loss improved from 2.38259 to 2.37128, saving model to weights/caption-lda-weights.05-2.37.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'topic_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "755/755 [==============================] - 413s 547ms/step - loss: 2.1952 - val_loss: 2.3713\n",
      "Epoch 6/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.1169\n",
      "Epoch 00006: val_loss improved from 2.37128 to 2.37124, saving model to weights/caption-lda-weights.06-2.37.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'topic_lstm/while/Exit_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "755/755 [==============================] - 416s 550ms/step - loss: 2.1170 - val_loss: 2.3712\n",
      "Epoch 7/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.0608\n",
      "Epoch 00007: val_loss did not improve from 2.37124\n",
      "755/755 [==============================] - 414s 548ms/step - loss: 2.0607 - val_loss: 2.3868\n",
      "Epoch 8/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 2.0180\n",
      "Epoch 00008: val_loss did not improve from 2.37124\n",
      "755/755 [==============================] - 414s 549ms/step - loss: 2.0180 - val_loss: 2.4088\n",
      "Epoch 9/30\n",
      "754/755 [============================>.] - ETA: 0s - loss: 1.9759\n",
      "Epoch 00009: val_loss did not improve from 2.37124\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "755/755 [==============================] - 414s 549ms/step - loss: 1.9759 - val_loss: 2.4093\n",
      "Epoch 10/30\n",
      "347/755 [============>.................] - ETA: 3:33 - loss: 1.9217"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(\n",
    "    generator=generator_train,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=generator_val,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
