{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding, Add, Reshape, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.utils import load_coco, load_image, print_progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the datset\n",
    "data_dir = 'dataset/processed_all_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type, data_dir):\n",
    "    # Path for the cache-file.\n",
    "    feature_cache_path = os.path.join(\n",
    "        data_dir, 'feature_transfer_values_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    captions_cache_path = os.path.join(\n",
    "        data_dir, 'captions_{}.pkl'.format(data_type)\n",
    "    )\n",
    "\n",
    "    feature_path_exists = os.path.exists(feature_cache_path)\n",
    "    caption_path_exists = os.path.exists(captions_cache_path)\n",
    "    if feature_path_exists and caption_path_exists:\n",
    "        with open(feature_cache_path, mode='rb') as file:\n",
    "            feature_obj = pickle.load(file)\n",
    "        with open(captions_cache_path, mode='rb') as file:\n",
    "            captions = pickle.load(file)\n",
    "    else:\n",
    "        sys.exit('processed {} data does not exist.'.format(data_type))\n",
    "    \n",
    "    if data_type != 'test':\n",
    "        topics_cache_path = os.path.join(\n",
    "            data_dir, 'topics_{}.pkl'.format(data_type)\n",
    "        )\n",
    "        if os.path.exists(topics_cache_path):\n",
    "            with open(topics_cache_path, mode='rb') as file:\n",
    "                topics = pickle.load(file)\n",
    "            print('{} data loaded from cache-file.'.format(data_type))\n",
    "            return feature_obj, topics, captions\n",
    "\n",
    "    print('{} data loaded from cache-file.'.format(data_type))\n",
    "    return feature_obj, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from cache-file.\n",
      "val data loaded from cache-file.\n",
      "test data loaded from cache-file.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-processed data\n",
    "features_train, topics_train, captions_train = load_data(\n",
    "    'train', data_dir\n",
    ")\n",
    "features_val, topics_val, captions_val = load_data(\n",
    "    'val', data_dir\n",
    ")\n",
    "features_test, captions_test = load_data(\n",
    "    'test', data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_captions(captions_list, mark_start, mark_end):\n",
    "    \"\"\" Mark all the captions with the start and the end marker \"\"\"\n",
    "    captions_marked = [\n",
    "        [' '.join([mark_start, caption, mark_end]) for caption in captions] for captions in captions_list\n",
    "    ]\n",
    "    \n",
    "    return captions_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(captions_list):\n",
    "    \"\"\" Flatten all the captions into a single list \"\"\"\n",
    "    caption_list = [\n",
    "        caption for caption_list in captions_list for caption in caption_list\n",
    "    ]\n",
    "    \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(captions_marked):\n",
    "    captions_flat = flatten(captions_marked)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(captions_flat)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'startseq'\n",
    "mark_end = 'endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked captions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['startseq A bird perched on the pavement and a lady seated near it endseq',\n",
       " 'startseq A black bird perched on ledge next to a tree. endseq',\n",
       " 'startseq A black bird sits on the corner of a wall. endseq',\n",
       " 'startseq A gray bird stands on a rocky bench near some leaves. endseq',\n",
       " 'startseq A bird sitting on a small cement bench looking out at something. endseq']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train_marked = mark_captions(captions_train, mark_start, mark_end)  # training\n",
    "captions_val_marked = mark_captions(captions_val, mark_start, mark_end)  # validation\n",
    "\n",
    "print('Marked captions:')\n",
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, vocab_size = create_tokenizer(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.num_words = 1000\n",
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of each caption\n",
    "max_tokens = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, topic_transfer_value, feature_transfer_value, caption, vocab_size):\n",
    "    \"\"\" Create sequences of topic_values, feature_values, input sequence and output sequence for an image \"\"\"\n",
    "    topic_values, feature_values = [], []\n",
    "    input_captions, output_captions = [], []\n",
    "    integer_sequence = tokenizer.texts_to_sequences([caption])[0]  # encode the sequence\n",
    "    \n",
    "    for idx in range(1, len(integer_sequence)):\n",
    "        in_seq, out_seq = integer_sequence[:idx], integer_sequence[idx]  # split into input and output pair\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post', truncating='post')[0]  # pad input sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]  # encode output sequence\n",
    "        \n",
    "        # store\n",
    "        topic_values.append(topic_transfer_value)\n",
    "        feature_values.append(feature_transfer_value)\n",
    "        input_captions.append(in_seq)\n",
    "        output_captions.append(out_seq)\n",
    "        \n",
    "    return topic_values, feature_values, input_captions, output_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(\n",
    "    topic_transfer_values, feature_transfer_values, captions_list, tokenizer, num_images, batch_size, max_length, vocab_size\n",
    "):\n",
    "    \"\"\" Generator function for creating random batches of training-data \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Get a list of random indices for images in the dataset.\n",
    "        indices = np.random.randint(num_images, size=batch_size)\n",
    "        \n",
    "        # For a batch of the randomly chosen images there are\n",
    "        # at least 5 captions describing the contents of the image.\n",
    "        # Select one of those captions at random\n",
    "        topic_values, feature_values = [], []\n",
    "        input_captions, output_captions = [], []\n",
    "        for idx in indices:\n",
    "            topic_value, feature_value, input_caption, output_caption = create_sequences(\n",
    "                tokenizer,\n",
    "                max_length,\n",
    "                topic_transfer_values[idx],\n",
    "                feature_transfer_values[idx],\n",
    "                np.random.choice(captions_list[idx]),\n",
    "                vocab_size\n",
    "            )\n",
    "            topic_values.extend(topic_value)\n",
    "            feature_values.extend(feature_value)\n",
    "            input_captions.extend(input_caption)\n",
    "            output_captions.extend(output_caption)\n",
    "\n",
    "        # Dict for the input-data. Because we have\n",
    "        # several inputs, we use a named dict to\n",
    "        # ensure that the data is assigned correctly.\n",
    "        x_data = {\n",
    "            'caption_input': np.array(input_captions),\n",
    "            'topic_input': np.array(topic_values),\n",
    "            'feature_input': np.array(feature_values)\n",
    "        }\n",
    "\n",
    "        # Dict for the output-data.\n",
    "        y_data = {\n",
    "            'caption_output': np.array(output_captions)\n",
    "        }\n",
    "        \n",
    "        yield (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training-dataset generator\n",
    "generator_train = batch_generator(\n",
    "    topics_train,\n",
    "    features_train,\n",
    "    captions_train_marked,\n",
    "    tokenizer,\n",
    "    len(captions_train),\n",
    "    batch_size,\n",
    "    max_tokens,\n",
    "    vocab_size\n",
    ")\n",
    "\n",
    "# validation-dataset generator\n",
    "generator_val = batch_generator(\n",
    "    topics_val,\n",
    "    features_val,\n",
    "    captions_val_marked,\n",
    "    tokenizer,\n",
    "    len(captions_val),\n",
    "    batch_size,\n",
    "    max_tokens,\n",
    "    vocab_size\n",
    ")\n",
    "\n",
    "batch = next(generator_train)\n",
    "batch_x = batch[0]\n",
    "batch_y = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00185185, 0.09444444, 0.00185185, 0.02037037, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.26111111, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.37222222, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.05740741, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.05740741, 0.00185185, 0.00185185,\n",
       "       0.00185185, 0.00185185, 0.00185185, 0.00185185, 0.00185185])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the topic-transfer-values for the first image in the batch\n",
    "batch_x['topic_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4600898 , 0.        , 0.60919714, ..., 0.        , 0.53796935,\n",
       "       0.27270582], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the feature-transfer-values for the first image in the batch\n",
    "batch_x['feature_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the token-sequence for the first image in the batch\n",
    "# This is the input to the decoder-part of the neural network\n",
    "batch_x['caption_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the token-sequence for the output of the decoder\n",
    "# Note how it is the same as the sequence above, except it is shifted one time-step\n",
    "batch_y['caption_output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the output\n",
    "batch_y['caption_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steps_per_epoch(captions_list, batch_size):\n",
    "    # Number of captions for each image\n",
    "    num_captions = [len(captions) for captions in captions_list]\n",
    "    \n",
    "    # Total number of captions\n",
    "    total_num_captions = np.sum(num_captions)\n",
    "    \n",
    "    return int(total_num_captions / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4385"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_train = calculate_steps_per_epoch(captions_train_marked, batch_size)\n",
    "steps_per_epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_val = calculate_steps_per_epoch(captions_val_marked, batch_size)\n",
    "steps_per_epoch_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pre-trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.embedding_layer import create_embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal state-sizes of LSTMs\n",
    "state_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encoder(feature_shape, state_size):\n",
    "    \"\"\" Encode Images \"\"\"\n",
    "    feature_input = Input(\n",
    "        shape=feature_shape, name='feature_input'\n",
    "    )\n",
    "    feature_net = Dropout(0.5)(feature_input)\n",
    "    image_model_output = Dense(state_size, activation='relu', name='image_model_output')(feature_net)\n",
    "    return feature_input, image_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_input, image_model_output = image_encoder(features_train.shape[1:], state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model to encode captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_encoder(\n",
    "    topic_input_shape, word_index, glove_file, mark_start, mark_end, state_size, vocab_size, max_tokens\n",
    "):\n",
    "    \"\"\" Encode Captions \"\"\"\n",
    "\n",
    "    # Define layers\n",
    "    topic_input = Input(\n",
    "        shape=topic_input_shape, name='topic_input'\n",
    "    )\n",
    "    topic_input_reshaped = Dense(state_size, activation='tanh', name='topic_input_reshaped')(topic_input)\n",
    "    caption_input = Input(shape=(max_tokens,), name='caption_input')\n",
    "    caption_embedding = create_embedding_layer(word_index, glove_file, mark_start, mark_end, vocab_size)\n",
    "    caption_lstm = GRU(state_size, name='caption_lstm')\n",
    "\n",
    "    # connect layers\n",
    "    net = caption_input  # Start the decoder-network with its input-layer\n",
    "    net = caption_embedding(net)  # Connect the embedding-layer\n",
    "    net = Dropout(0.5)(net)\n",
    "    caption_model_output = caption_lstm(net, initial_state=topic_input_reshaped) # Connect the caption LSTM layer\n",
    "\n",
    "    return topic_input, caption_input, caption_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word to vec map...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Encode Captions\n",
    "glove_file = 'dataset/glove.6B.300d.txt'\n",
    "topic_input, caption_input, caption_model_output = caption_encoder(\n",
    "    topics_train.shape[1:],\n",
    "    tokenizer.word_index,\n",
    "    glove_file,\n",
    "    mark_start,\n",
    "    mark_end,\n",
    "    state_size,\n",
    "    len(tokenizer.word_index) + 1,\n",
    "    max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the image and the caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the outputs of both the models\n",
    "merge_net = Add()([image_model_output, caption_model_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caption_output/Softmax:0' shape=(?, 1000) dtype=float32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_net = Dense(state_size, activation='relu')(merge_net)\n",
    "outputs = Dense(vocab_size, activation='softmax', name='caption_output')(merge_net)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[feature_input, topic_input, caption_input],\n",
    "    outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_input (InputLayer)      (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, 16, 300)      8006400     caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "topic_input (InputLayer)        (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           feature_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 300)      0           decoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "topic_input_reshaped (Dense)    (None, 256)          20736       topic_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "image_model_output (Dense)      (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "caption_lstm (GRU)              (None, 256)          427776      dropout_1[0][0]                  \n",
      "                                                                 topic_input_reshaped[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           image_model_output[0][0]         \n",
      "                                                                 caption_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "caption_output (Dense)          (None, 1000)         257000      dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,826,536\n",
      "Trainable params: 1,820,136\n",
      "Non-trainable params: 8,006,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'weights/cplda-weights-{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    filepath=path_checkpoint,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(\n",
    "    log_dir='./weights/caption-lda-logs/',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stop = EarlyStopping(monitor='val_loss', patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_checkpoint, callback_tensorboard, callback_early_stop, callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights('weights/')\n",
    "    print('Weights loaded.')\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='caption_model_dropout.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "4384/4385 [============================>.] - ETA: 0s - loss: 2.7081\n",
      "Epoch 00001: val_loss improved from inf to 3.21933, saving model to weights/cplda-weights-01-3.22.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_input_reshaped/Tanh:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "4385/4385 [==============================] - 265s 60ms/step - loss: 2.7080 - val_loss: 3.2193\n",
      "Epoch 2/60\n",
      "4384/4385 [============================>.] - ETA: 0s - loss: 2.2447\n",
      "Epoch 00002: val_loss improved from 3.21933 to 3.20960, saving model to weights/cplda-weights-02-3.21.hdf5\n",
      "WARNING:tensorflow:Layer caption_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'topic_input_reshaped/Tanh:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "4385/4385 [==============================] - 262s 60ms/step - loss: 2.2447 - val_loss: 3.2096\n",
      "Epoch 3/60\n",
      " 117/4385 [..............................] - ETA: 3:42 - loss: 2.1940"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(\n",
    "    generator=generator_train,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=60,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=generator_val,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
