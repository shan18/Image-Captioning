{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the datset\n",
    "data_dir = 'dataset/processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, data_dir, data_type):\n",
    "    h5f = h5py.File(os.path.join(data_dir, filename), 'r')\n",
    "    data = h5f[data_type][:]\n",
    "    h5f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = load_data('train_images.h5', data_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = load_data('train_categories.h5', data_dir, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = load_data('val_images.h5', data_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categories = load_data('val_categories.h5', data_dir, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = load_data('test_images.h5', data_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_categories = load_data('test_categories.h5', data_dir, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/coco_raw.pickle', 'rb') as file:\n",
    "    coco_raw = pickle.load(file)\n",
    "id_category = coco_raw['id_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(id_category)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_train = train_images.shape[0]\n",
    "num_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image\n",
    "idx = 10\n",
    "\n",
    "for category, value in enumerate(train_categories[idx]):\n",
    "    if value != 0:\n",
    "        print(id_category[category])\n",
    "\n",
    "plt.imshow(train_images[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VGG19 model along with the fully-connected layers\n",
    "model = VGG19(include_top=True, weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second last fully connected layer\n",
    "conv_layer = model.get_layer('fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "conv_model = Model(inputs=model.input, outputs=conv_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new Keras Sequential model\n",
    "image_model = Sequential()\n",
    "\n",
    "# Add the VGG19 model\n",
    "image_model.add(conv_model)\n",
    "\n",
    "# Add the final layer for the actual classification\n",
    "image_model.add(Dense(num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the VGG19 layers to be non-trainable\n",
    "conv_model.trainable = False\n",
    "for layer in conv_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(lr=1e-3)\n",
    "\n",
    "image_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'weights'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "path_checkpoint = os.path.join(weights_dir, 'checkpoint.keras')\n",
    "\n",
    "# set model callbacks\n",
    "tb = TensorBoard(log_dir=os.path.join(weights_dir, 'tensorboard-logs'), histogram_freq=0, write_graph=False)\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, verbose=1, save_weights_only=True)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "callbacks = [tb, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained model weights\n",
    "try:\n",
    "    image_model.load_weights(path_checkpoint)\n",
    "    print('Checkpoint loaded.')\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.fit(\n",
    "    x=train_images,\n",
    "    y=train_categories,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(val_images, val_categories)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_data_generator(x, y, batch_size, shift_fraction=0.2, rotation_range=40, shear_range=0.2):\n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#         rotation_range=rotation_range,\n",
    "#         width_shift_range=shift_fraction,\n",
    "#         height_shift_range=shift_fraction,\n",
    "#         shear_range=shear_range\n",
    "#     )\n",
    "#     generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "#     while True:\n",
    "#         x_batch, y_batch = generator.next()\n",
    "#         yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_model.fit_generator(\n",
    "#     generator=train_data_generator(train_images, train_categories, batch_size),\n",
    "#     steps_per_epoch=int(num_images_train / batch_size),\n",
    "#     epochs=100,\n",
    "#     validation_data=(val_images, val_categories),\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(image, label, id_category):\n",
    "    image_batch = np.expand_dims(image, axis=0)\n",
    "    predictions = image_model.predict(image_batch)\n",
    "    \n",
    "    prediction_labels = []\n",
    "    for index, prediction_probability in enumerate(predictions[0]):\n",
    "        if prediction_probability > 0.5:\n",
    "            prediction_labels.append(id_category[index])\n",
    "    \n",
    "    true_labels = []\n",
    "    for index, value in enumerate(label):\n",
    "        if value == 1:\n",
    "            true_labels.append(id_category[index])\n",
    "    \n",
    "    print('True labels:', true_labels)\n",
    "    print('Predictions:', prediction_labels)\n",
    "    \n",
    "    print('Image:')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(test_images[idx], test_categories[idx], id_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(val_images[idx], val_categories[idx], id_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(train_images[idx], train_categories[idx], id_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
