{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.caption_model import create_model\n",
    "from dataset.process_texts import (\n",
    "    mark_captions,\n",
    "    clean_captions,\n",
    "    caption_to_sequence,\n",
    "    build_vocabulary_with_frequency_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the datset\n",
    "data_dir = 'dataset/processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type, data_dir):\n",
    "    # Path for the cache-file.\n",
    "    feature_cache_path = os.path.join(\n",
    "        data_dir, 'features_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    topics_cache_path = os.path.join(\n",
    "        data_dir, 'topics_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    captions_cache_path = os.path.join(\n",
    "        data_dir, 'captions_{}.pkl'.format(data_type)\n",
    "    )\n",
    "\n",
    "    feature_path_exists = os.path.exists(feature_cache_path)\n",
    "    topic_path_exists = os.path.exists(topics_cache_path)\n",
    "    caption_path_exists = os.path.exists(captions_cache_path)\n",
    "    if feature_path_exists and topic_path_exists and caption_path_exists:\n",
    "        with open(feature_cache_path, mode='rb') as file:\n",
    "            feature_obj = pickle.load(file)\n",
    "        with open(topics_cache_path, mode='rb') as file:\n",
    "            topics = pickle.load(file)\n",
    "        with open(captions_cache_path, mode='rb') as file:\n",
    "            captions = pickle.load(file)\n",
    "    else:\n",
    "        sys.exit('processed {} data does not exist.'.format(data_type))\n",
    "\n",
    "    print('{} data loaded from cache-file.'.format(data_type))\n",
    "    return feature_obj, topics, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_captions(captions_list, mark_start, mark_end, freq_threshold):\n",
    "    captions_list_marked = mark_captions(captions_list, mark_start, mark_end)\n",
    "    captions_list_marked = clean_captions(captions_list_marked)\n",
    "    vocab, word_idx, _ = build_vocabulary_with_frequency_threshold(captions_list_marked, freq_threshold)\n",
    "    return captions_list_marked, word_idx, len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steps_per_epoch(captions_list, batch_size):\n",
    "    # Number of captions for each image\n",
    "    num_captions = [len(captions) for captions in captions_list]\n",
    "    \n",
    "    # Total number of captions\n",
    "    total_num_captions = np.sum(num_captions)\n",
    "    \n",
    "    return int(total_num_captions / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(word_idx, max_length, topic_transfer_value, feature_transfer_value, caption, vocab_size):\n",
    "    \"\"\" Create sequences of topic_values, feature_values, input sequence and output sequence for an image \"\"\"\n",
    "    topic_values, feature_values = [], []\n",
    "    input_captions, output_captions = [], []\n",
    "    integer_sequence = caption_to_sequence(caption, word_idx)  # encode the sequence\n",
    "    \n",
    "    for idx in range(1, len(integer_sequence)):\n",
    "        in_seq, out_seq = integer_sequence[:idx], integer_sequence[idx]  # split into input and output pair\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post', truncating='post')[0]  # pad input sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]  # encode output sequence\n",
    "        \n",
    "        # store\n",
    "        topic_values.append(topic_transfer_value)\n",
    "        feature_values.append(feature_transfer_value)\n",
    "        input_captions.append(in_seq)\n",
    "        output_captions.append(out_seq)\n",
    "        \n",
    "    return topic_values, feature_values, input_captions, output_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(topic_transfer_values, feature_transfer_values, captions_list, word_idx, num_images, batch_size, max_length, vocab_size):\n",
    "    while True:\n",
    "        indices = np.random.randint(num_images, size=batch_size)\n",
    "        topic_values, feature_values = [], []\n",
    "        input_captions, output_captions = [], []\n",
    "        for idx in indices:\n",
    "            topic_value, feature_value, input_caption, output_caption = create_sequences(\n",
    "                word_idx,\n",
    "                max_length,\n",
    "                topic_transfer_values[idx],\n",
    "                feature_transfer_values[idx],\n",
    "                np.random.choice(captions_list[idx]),\n",
    "                vocab_size\n",
    "            )\n",
    "            topic_values.extend(topic_value)\n",
    "            feature_values.extend(feature_value)\n",
    "            input_captions.extend(input_caption)\n",
    "            output_captions.extend(output_caption)\n",
    "            \n",
    "        x_data = {\n",
    "            'caption_input': np.array(input_captions),\n",
    "            'topic_input': np.array(topic_values),\n",
    "            'feature_input': np.array(feature_values)\n",
    "        }\n",
    "\n",
    "        y_data = {\n",
    "            'caption_output': np.array(output_captions)\n",
    "        }\n",
    "        \n",
    "        yield (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from cache-file.\n",
      "val data loaded from cache-file.\n",
      "\n",
      "Features shape: (112218, 1000)\n",
      "Topics shape: (112218, 80)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-processed data\n",
    "features_train, topics_train, captions_train = load_data(\n",
    "    'train', data_dir\n",
    ")\n",
    "features_val, topics_val, captions_val = load_data(\n",
    "    'val', data_dir\n",
    ")\n",
    "print('\\nFeatures shape:', features_train.shape)\n",
    "print('Topics shape:', topics_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process captions\n",
    "mark_start = 'startseq'\n",
    "mark_end = 'endseq'\n",
    "captions_train_marked, word_idx, vocab_size = process_captions(  # training\n",
    "    captions_train, mark_start, mark_end, 10\n",
    ")\n",
    "captions_val_marked = mark_captions(captions_val, mark_start, mark_end)  # validation\n",
    "captions_val_marked = clean_captions(captions_val_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training-dataset generator\n",
    "generator_train = batch_generator(\n",
    "    topics_train,\n",
    "    features_train,\n",
    "    captions_train_marked,\n",
    "    word_idx,\n",
    "    len(captions_train),\n",
    "    256,\n",
    "    16,\n",
    "    vocab_size\n",
    ")\n",
    "\n",
    "# validation-dataset generator\n",
    "generator_val = batch_generator(\n",
    "    topics_val,\n",
    "    features_val,\n",
    "    captions_val_marked,\n",
    "    word_idx,\n",
    "    len(captions_val),\n",
    "    256,\n",
    "    16,\n",
    "    vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6857"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word to vec map...\n",
      "Done!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "topic_input (InputLayer)        (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_input (InputLayer)      (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, 16, 300)      2057400     caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 80, 1)        0           topic_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           feature_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 300)      0           decoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "topic_lstm (LSTM)               [(None, 256), (None, 264192      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "image_model_output (Dense)      (None, 256)          256256      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "caption_lstm (LSTM)             (None, 256)          570368      dropout_1[0][0]                  \n",
      "                                                                 topic_lstm[0][1]                 \n",
      "                                                                 topic_lstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           image_model_output[0][0]         \n",
      "                                                                 caption_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "caption_output (Dense)          (None, 6857)         1762249     dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,976,257\n",
      "Trainable params: 2,918,857\n",
      "Non-trainable params: 2,057,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "model = create_model(\n",
    "    topics_train.shape[1:],\n",
    "    features_train.shape[1:],\n",
    "    word_idx,\n",
    "    'dataset/glove.6B.300d.txt',\n",
    "    mark_start,\n",
    "    mark_end,\n",
    "    vocab_size,\n",
    "    16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "path_checkpoint = 'weights/cplda-weights-{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    filepath=path_checkpoint,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "callback_tensorboard = TensorBoard(\n",
    "    log_dir='./weights/caption-lda-logs/',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True\n",
    ")\n",
    "callback_early_stop = EarlyStopping(monitor='val_loss', patience=25, verbose=1)\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, min_lr=0.00001)\n",
    "callbacks = [callback_checkpoint, callback_tensorboard, callback_early_stop, callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  71/4385 [..............................] - ETA: 28:28 - loss: 6.2030"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a7a144d0e57e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_steps_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.fit_generator(\n",
    "    generator=generator_train,\n",
    "    steps_per_epoch=calculate_steps_per_epoch(captions_train, 128),\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=generator_val,\n",
    "    validation_steps=calculate_steps_per_epoch(captions_val, 128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
