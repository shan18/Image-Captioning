{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Add, Reshape, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.utils import load_coco, load_image, print_progress_bar\n",
    "from models.vgg19 import load_vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the datset\n",
    "data_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, category_id, id_category = load_coco(\n",
    "    os.path.join(data_dir, 'coco_raw.pickle'), 'captions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_captions = train_data  # Load training data\n",
    "val_images, val_captions = val_data  # Load validation data\n",
    "test_images, test_captions = test_data  # Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(id_category)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_images_train = len(train_images)\n",
    "num_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_images_val = len(val_images)\n",
    "num_images_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained feature extractor model\n",
    "feature_model = load_vgg19()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(feature_model, data_dir, filenames, batch_size):\n",
    "    \"\"\"\n",
    "    Process all the given files in the given data_dir using the\n",
    "    pre-trained feature-model as well as the feature-model and return\n",
    "    their transfer-values.\n",
    "    \n",
    "    The images are processed in batches to save\n",
    "    memory and improve efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_images = len(filenames)\n",
    "    img_size = K.int_shape(feature_model.input)[1:3]    # Expected input size of the pre-trained network\n",
    "\n",
    "    # Pre-allocate input-batch-array for images.\n",
    "    shape = (batch_size,) + img_size + (3,)\n",
    "    image_batch = np.zeros(shape=shape, dtype=np.float32)\n",
    "\n",
    "    # Pre-allocate output-array for transfer-values.\n",
    "    feature_transfer_values = np.zeros(\n",
    "        shape=(num_images, K.int_shape(feature_model.output)[1]),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    start_index = 0\n",
    "    print_progress_bar(start_index, num_images)  # Initial call to print 0% progress\n",
    "\n",
    "    while start_index < num_images:\n",
    "        end_index = start_index + batch_size\n",
    "        if end_index > num_images:\n",
    "            end_index = num_images\n",
    "        current_batch_size = end_index - start_index\n",
    "\n",
    "        # Load all the images in the batch.\n",
    "        for i, filename in enumerate(filenames[start_index:end_index]):\n",
    "            path = os.path.join(data_dir, filename)\n",
    "            img = load_image(path, size=img_size)\n",
    "            image_batch[i] = img\n",
    "\n",
    "        # Use the pre-trained models to process the image.\n",
    "        feature_transfer_values_batch = feature_model.predict(\n",
    "            image_batch[0:current_batch_size]\n",
    "        )\n",
    "\n",
    "        # Save the transfer-values in the pre-allocated array.\n",
    "        feature_transfer_values[start_index:end_index] = feature_transfer_values_batch[0:current_batch_size]\n",
    "\n",
    "        start_index = end_index\n",
    "        print_progress_bar(start_index, num_images)  # Update Progress Bar\n",
    "\n",
    "    print()\n",
    "    return feature_transfer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(feature_model, data_dir, data_type, filenames, captions, batch_size):\n",
    "    print('Processing {0} images in {1}-set ...'.format(len(filenames), data_type))\n",
    "\n",
    "    # Path for the cache-file.\n",
    "    cache_path_dir = os.path.join(data_dir, 'processed_lda_data')\n",
    "    feature_cache_path = os.path.join(\n",
    "        cache_path_dir, 'feature_transfer_values_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    images_cache_path = os.path.join(\n",
    "        cache_path_dir, 'images_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    captions_cache_path = os.path.join(\n",
    "        cache_path_dir, 'captions_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    \n",
    "    # Check if directory to store processed data exists\n",
    "    if not os.path.exists(cache_path_dir):\n",
    "        print('Directory created:', cache_path_dir)\n",
    "        os.mkdir(cache_path_dir)\n",
    "\n",
    "    # If the cache-file already exists then reload it,\n",
    "    # otherwise process all images and save their transfer-values\n",
    "    # to the cache-file so it can be reloaded quickly.\n",
    "    feature_path_exists = os.path.exists(feature_cache_path)\n",
    "    image_path_exists = os.path.exists(images_cache_path)\n",
    "    caption_path_exists = os.path.exists(captions_cache_path)\n",
    "    if feature_path_exists and image_path_exists and caption_path_exists:\n",
    "        with open(feature_cache_path, mode='rb') as file:\n",
    "            feature_obj = pickle.load(file)\n",
    "        with open(images_cache_path, mode='rb') as file:\n",
    "            filenames = pickle.load(file)\n",
    "        with open(captions_cache_path, mode='rb') as file:\n",
    "            captions = pickle.load(file)\n",
    "        print(\"Data loaded from cache-file.\")\n",
    "    else:\n",
    "        feature_obj = process_images(\n",
    "            feature_model, data_dir, filenames, batch_size\n",
    "        )\n",
    "        with open(feature_cache_path, mode='wb') as file:\n",
    "            pickle.dump(feature_obj, file)\n",
    "        with open(images_cache_path, mode='wb') as file:\n",
    "            pickle.dump(filenames, file)\n",
    "        with open(captions_cache_path, mode='wb') as file:\n",
    "            pickle.dump(captions, file)\n",
    "        print(\"Data saved to cache-file.\")\n",
    "\n",
    "    return feature_obj, filenames, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 19324 images in train-set ...\n",
      "Data loaded from cache-file.\n",
      "feature shape: (19324, 4096)\n",
      "CPU times: user 108 ms, sys: 345 ms, total: 453 ms\n",
      "Wall time: 450 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training Data\n",
    "feature_transfer_values_train, images_train, captions_train = process_data(\n",
    "    feature_model, data_dir, 'train', train_images, train_captions, process_batch_size\n",
    ")\n",
    "print(\"feature shape:\", feature_transfer_values_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2415 images in val-set ...\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "\n",
      "Data saved to cache-file.\n",
      "feature shape: (2415, 4096)\n",
      "CPU times: user 15min 1s, sys: 8.57 s, total: 15min 9s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Validation Data\n",
    "feature_transfer_values_val, images_val, captions_val = process_data(\n",
    "    feature_model, data_dir, 'val', val_images, val_captions, process_batch_size\n",
    ")\n",
    "print(\"feature shape:\", feature_transfer_values_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2417 images in test-set ...\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "\n",
      "Data saved to cache-file.\n",
      "feature shape: (2417, 4096)\n",
      "CPU times: user 14min 33s, sys: 8.93 s, total: 14min 42s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test Data\n",
    "feature_transfer_values_test, images_test, captions_test = process_data(\n",
    "    feature_model, data_dir, 'test', test_images, test_captions, process_batch_size\n",
    ")\n",
    "print(\"feature shape:\", feature_transfer_values_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'startseq'\n",
    "mark_end = 'endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_captions(captions_list):\n",
    "    \"\"\" Mark all the captions with the start and the end marker \"\"\"\n",
    "    captions_marked = [\n",
    "        [' '.join([mark_start, caption, mark_end]) for caption in captions] for captions in captions_list\n",
    "    ]\n",
    "    \n",
    "    return captions_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked captions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['startseq Children playing and standing on a small tennis court. endseq',\n",
       " 'startseq A group of children standing on a tennis court. endseq',\n",
       " 'startseq Children learning to play tennis with instructors at an outdoor tennis court. endseq',\n",
       " 'startseq Several children gathered on a tennis court learning how today tennis endseq',\n",
       " 'startseq A group of children are gathered around playing tennis. endseq']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train_marked = mark_captions(captions_train)  # training\n",
    "captions_val_marked = mark_captions(captions_val)  # validation\n",
    "print('Marked captions:')\n",
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(captions_list):\n",
    "    \"\"\" Flatten all the captions into a single list \"\"\"\n",
    "    caption_list = [caption\n",
    "                    for caption_list in captions_list\n",
    "                    for caption in caption_list]\n",
    "    \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_flat = flatten(captions_train_marked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 7.96 ms, total: 1.95 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get integer token for the start marker\n",
    "token_start = tokenizer.word_index[mark_start]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get integer token for the end marker\n",
    "token_end = tokenizer.word_index[mark_end]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8632"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numbers of words in the vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of each caption\n",
    "max_tokens = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
