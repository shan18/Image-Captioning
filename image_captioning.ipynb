{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Add, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.utils import load_coco, load_image, print_progress_bar\n",
    "from image_model.topic_layers import load_topic_model, load_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the datset\n",
    "data_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, category_id, id_category = load_coco(\n",
    "    os.path.join(data_dir, 'coco_raw.pickle'), 'captions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_captions = train_data  # Load training data\n",
    "val_images, val_captions = val_data  # Load validation data\n",
    "test_images, test_captions = test_data  # Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(id_category)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_train = len(train_images)\n",
    "num_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_val = len(val_images)\n",
    "num_images_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image\n",
    "idx = 10\n",
    "\n",
    "for caption in train_captions[idx]:\n",
    "    print(caption)\n",
    "\n",
    "plt.imshow(load_image(os.path.join(data_dir, train_images[idx])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained topic model\n",
    "weights_path = os.path.join(os.getcwd(), 'topic_extraction', 'weights', 'checkpoint.keras')\n",
    "topic_model = load_topic_model(num_classes, weights_path)\n",
    "topic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained feature extractor model\n",
    "feature_model = load_feature_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(topic_model, feature_model, data_dir, filenames, batch_size):\n",
    "    \"\"\"\n",
    "    Process all the given files in the given data_dir using the\n",
    "    pre-trained topic-model as well as the feature-model and return\n",
    "    their transfer-values.\n",
    "    \n",
    "    The images are processed in batches to save\n",
    "    memory and improve efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_images = len(filenames)\n",
    "    img_size = K.int_shape(topic_model.input)[1:3]    # Expected input size of the pre-trained network\n",
    "\n",
    "    # Pre-allocate input-batch-array for images.\n",
    "    shape = (batch_size,) + img_size + (3,)\n",
    "    image_batch = np.zeros(shape=shape, dtype=np.float32)\n",
    "\n",
    "    # Pre-allocate output-array for transfer-values.\n",
    "    topic_transfer_values = np.zeros(\n",
    "        shape=(num_images,) + K.int_shape(topic_model.output)[1:],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    feature_transfer_values = np.zeros(\n",
    "        shape=(num_images, K.int_shape(feature_model.output)[1]),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    start_index = 0\n",
    "    print_progress_bar(start_index, num_images)  # Initial call to print 0% progress\n",
    "\n",
    "    while start_index < num_images:\n",
    "        end_index = start_index + batch_size\n",
    "        if end_index > num_images:\n",
    "            end_index = num_images\n",
    "        current_batch_size = end_index - start_index\n",
    "\n",
    "        # Load all the images in the batch.\n",
    "        for i, filename in enumerate(filenames[start_index:end_index]):\n",
    "            path = os.path.join(data_dir, filename)\n",
    "            img = load_image(path, size=img_size)\n",
    "            image_batch[i] = img\n",
    "\n",
    "        # Use the pre-trained models to process the image.\n",
    "        topic_transfer_values_batch = topic_model.predict(\n",
    "            image_batch[0:current_batch_size]\n",
    "        )\n",
    "        feature_transfer_values_batch = feature_model.predict(\n",
    "            image_batch[0:current_batch_size]\n",
    "        )\n",
    "\n",
    "        # Save the transfer-values in the pre-allocated arrays.\n",
    "        topic_transfer_values[start_index:end_index] = topic_transfer_values_batch[0:current_batch_size]\n",
    "        feature_transfer_values[start_index:end_index] = feature_transfer_values_batch[0:current_batch_size]\n",
    "\n",
    "        start_index = end_index\n",
    "        print_progress_bar(start_index, num_images)  # Update Progress Bar\n",
    "\n",
    "    print()\n",
    "    return topic_transfer_values, feature_transfer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(topic_model, feature_model, data_dir, data_type, filenames, captions, batch_size):\n",
    "    print('Processing {0} images in {1}-set ...'.format(len(filenames), data_type))\n",
    "\n",
    "    # Path for the cache-file.\n",
    "    cache_path_dir = os.path.join(data_dir, 'processed_caption_data')\n",
    "    topic_cache_path = os.path.join(\n",
    "        cache_path_dir, 'topic_transfer_values_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    feature_cache_path = os.path.join(\n",
    "        cache_path_dir, 'feature_transfer_values_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    images_cache_path = os.path.join(\n",
    "        cache_path_dir, 'images_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    captions_cache_path = os.path.join(\n",
    "        cache_path_dir, 'captions_{}.pkl'.format(data_type)\n",
    "    )\n",
    "    \n",
    "    # Check if directory to store processed data exists\n",
    "    if not os.path.exists(cache_path_dir):\n",
    "        print('Directory created:', cache_path_dir)\n",
    "        os.mkdir(cache_path_dir)\n",
    "\n",
    "    # If the cache-file already exists then reload it,\n",
    "    # otherwise process all images and save their transfer-values\n",
    "    # to the cache-file so it can be reloaded quickly.\n",
    "    topic_path_exists = os.path.exists(topic_cache_path)\n",
    "    feature_path_exists = os.path.exists(feature_cache_path)\n",
    "    image_path_exists = os.path.exists(images_cache_path)\n",
    "    caption_path_exists = os.path.exists(captions_cache_path)\n",
    "    if topic_path_exists and feature_path_exists and image_path_exists and caption_path_exists:\n",
    "        with open(topic_cache_path, mode='rb') as file:\n",
    "            topic_obj = pickle.load(file)\n",
    "        with open(feature_cache_path, mode='rb') as file:\n",
    "            feature_obj = pickle.load(file)\n",
    "        with open(images_cache_path, mode='rb') as file:\n",
    "            images = pickle.load(file)\n",
    "        with open(captions_cache_path, mode='rb') as file:\n",
    "            captions = pickle.load(file)\n",
    "        print(\"Data loaded from cache-file.\")\n",
    "    else:\n",
    "        topic_obj, feature_obj = process_images(\n",
    "            topic_model, feature_model, data_dir, filenames, batch_size\n",
    "        )\n",
    "        with open(topic_cache_path, mode='wb') as file:\n",
    "            pickle.dump(topic_obj, file)\n",
    "        with open(feature_cache_path, mode='wb') as file:\n",
    "            pickle.dump(feature_obj, file)\n",
    "        with open(images_cache_path, mode='wb') as file:\n",
    "            pickle.dump(filenames, file)\n",
    "        with open(captions_cache_path, mode='wb') as file:\n",
    "            pickle.dump(captions, file)\n",
    "        print(\"Data saved to cache-file.\")\n",
    "\n",
    "    return topic_obj, feature_obj, images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training Data\n",
    "topic_transfer_values_train, feature_transfer_values_train, images_train, captions_train = process_data(\n",
    "    topic_model, feature_model, data_dir, 'train', train_images, train_captions, process_batch_size\n",
    ")\n",
    "print(\"topic shape:\", topic_transfer_values_train.shape)\n",
    "print(\"feature shape:\", feature_transfer_values_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Validation Data\n",
    "topic_transfer_values_val, feature_transfer_values_val, images_val, captions_val = process_data(\n",
    "    topic_model, feature_model, data_dir, 'val', val_images, val_captions, process_batch_size\n",
    ")\n",
    "print(\"topic shape:\", topic_transfer_values_val.shape)\n",
    "print(\"feature shape:\", feature_transfer_values_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test Data\n",
    "topic_transfer_values_test, feature_transfer_values_test, images_test, captions_test = process_data(\n",
    "    topic_model, feature_model, data_dir, 'test', test_images, test_captions, process_batch_size\n",
    ")\n",
    "print(\"topic shape:\", topic_transfer_values_test.shape)\n",
    "print(\"feature shape:\", feature_transfer_values_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'startseq'\n",
    "mark_end = 'endseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_captions(captions_list):\n",
    "    \"\"\" Mark all the captions with the start and the end marker \"\"\"\n",
    "    captions_marked = [\n",
    "        [' '.join([mark_start, caption, mark_end]) for caption in captions] for captions in captions_list\n",
    "    ]\n",
    "    \n",
    "    return captions_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_marked = mark_captions(captions_train)\n",
    "print('Marked captions:')\n",
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(captions_list):\n",
    "    \"\"\" Flatten all the captions into a single list \"\"\"\n",
    "    caption_list = [caption\n",
    "                    for caption_list in captions_list\n",
    "                    for caption in caption_list]\n",
    "    \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_flat = flatten(captions_train_marked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get integer token for the start marker\n",
    "token_start = tokenizer.word_index[mark_start]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get integer token for the end marker\n",
    "token_end = tokenizer.word_index[mark_end]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers of words in the vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of each caption\n",
    "max_tokens = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, topic_transfer_value, feature_transfer_value, caption):\n",
    "    \"\"\" Create sequences of topic_values, feature_values, input sequence and output sequence for an image \"\"\"\n",
    "    topic_values, feature_values = [], []\n",
    "    input_captions, output_captions = [], []\n",
    "    integer_sequence = tokenizer.texts_to_sequences([caption])[0]  # encode the sequence\n",
    "    \n",
    "    for idx in range(1, len(integer_sequence)):\n",
    "        in_seq, out_seq = integer_sequence[:idx], integer_sequence[idx]  # split into input and output pair\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post', truncating='post')[0]  # pad input sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]  # encode output sequence\n",
    "        \n",
    "        # store\n",
    "        topic_values.append(topic_transfer_value)\n",
    "        feature_values.append(feature_transfer_value)\n",
    "        input_captions.append(in_seq)\n",
    "        output_captions.append(out_seq)\n",
    "        \n",
    "    return topic_values, feature_values, input_captions, output_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(topic_transfer_values, feature_transfer_values, captions_list, tokenizer, num_images, batch_size, max_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \n",
    "    It selects the data completely randomly for each\n",
    "    batch, corresponding to sampling of the training-set with\n",
    "    replacement. This means it is possible to sample the same\n",
    "    data multiple times within a single epoch - and it is also\n",
    "    possible that some data is not sampled at all within an epoch.\n",
    "    However, all the data should be unique within a single batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Get a list of random indices for images in the dataset.\n",
    "        indices = np.random.randint(num_images, size=batch_size)\n",
    "        \n",
    "        # For a batch of the randomly chosen images there are\n",
    "        # at least 5 captions describing the contents of the image.\n",
    "        # Select one of those captions at random\n",
    "        topic_values, feature_values = [], []\n",
    "        input_captions, output_captions = [], []\n",
    "        for idx in indices:\n",
    "            topic_value, feature_value, input_caption, output_caption = create_sequences(\n",
    "                tokenizer,\n",
    "                max_length,\n",
    "                topic_transfer_values[idx],\n",
    "                feature_transfer_values[idx],\n",
    "                np.random.choice(captions_list[idx])\n",
    "            )\n",
    "            topic_values.extend(topic_value)\n",
    "            feature_values.extend(feature_value)\n",
    "            input_captions.extend(input_caption)\n",
    "            output_captions.extend(output_caption)\n",
    "\n",
    "        # Dict for the input-data. Because we have\n",
    "        # several inputs, we use a named dict to\n",
    "        # ensure that the data is assigned correctly.\n",
    "        x_data = {\n",
    "            'caption_input': np.array(input_captions),\n",
    "            'topic_input': np.array(topic_values),\n",
    "            'feature_input': np.array(feature_values)\n",
    "        }\n",
    "\n",
    "        # Dict for the output-data.\n",
    "        y_data = {\n",
    "            'caption_output': np.array(output_captions)\n",
    "        }\n",
    "        \n",
    "        yield (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train = batch_generator(\n",
    "    topic_transfer_values_train,\n",
    "    feature_transfer_values_train,\n",
    "    captions_train_marked,\n",
    "    tokenizer,\n",
    "    num_images_train,\n",
    "    batch_size,\n",
    "    max_tokens,\n",
    "    vocab_size\n",
    ")\n",
    "batch = next(generator_train)\n",
    "batch_x = batch[0]\n",
    "batch_y = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the topic-transfer-values for the first image in the batch\n",
    "batch_x['topic_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the feature-transfer-values for the first image in the batch\n",
    "batch_x['feature_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the token-sequence for the first image in the batch\n",
    "# This is the input to the decoder-part of the neural network\n",
    "batch_x['caption_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the token-sequence for the output of the decoder\n",
    "# Note how it is the same as the sequence above, except it is shifted one time-step\n",
    "batch_y['caption_output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the output\n",
    "batch_y['caption_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Per Epoch\n",
    "\n",
    "One epoch is a complete processing of the training-set. We would like to process each image and caption pair only once per epoch. However, because each batch is chosen completely at random in the above batch-generator, it is possible that an image occurs in multiple batches within a single epoch, and it is possible that some images may not occur in any batch at all within a single epoch.\n",
    "\n",
    "Nevertheless, we still use the concept of an 'epoch' to measure approximately how many iterations of the training-data we have processed. But the data-generator will generate batches for eternity, so we need to manually calculate the approximate number of batches required per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of captions for each image in the training-set\n",
    "num_captions_train = [len(captions) for captions in captions_train]\n",
    "\n",
    "# Total number of captions in the training-set\n",
    "total_num_captions_train = np.sum(num_captions_train)\n",
    "\n",
    "# Approximate number of batches required per epoch,\n",
    "# if we want to process each caption and image pair once per epoch\n",
    "steps_per_epoch_train = int(total_num_captions_train / batch_size)\n",
    "steps_per_epoch_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pre-trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    print('Creating word to vec map...')\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n",
    "    print('Done!')\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "word_to_vec_map = read_glove_vecs('{}/glove.6B.300d.txt'.format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign embeddings values to custom tokens\n",
    "size = word_to_vec_map['unk'].shape\n",
    "\n",
    "word_to_vec_map[mark_start] = np.random.uniform(low=-1.0, high=1.0, size=size)\n",
    "word_to_vec_map[mark_end] = np.random.uniform(low=-1.0, high=1.0, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index, word_to_vec_map, num_words):\n",
    "    \"\"\" Create a Keras Embedding() layer and load in pre-trained GloVe 300-dimensional vectors\n",
    "        @params:\n",
    "        :word_to_index -- dictionary containing the each word mapped to its index\n",
    "        :word_to_vec_map -- dictionary mapping words to their GloVe vector representation\n",
    "        :num_words -- number of words in the vocabulary\n",
    "        \n",
    "        @return:\n",
    "        :decoder_embedding -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_length = num_words + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    embedding_dimensions = word_to_vec_map['unk'].shape[0]  # define dimensionality of GloVe word vectors (= 300)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocabulary_length, embedding_dimensions))  # initialize with zeros\n",
    "    for word, index in word_to_index.items():\n",
    "        try:\n",
    "            embedding_matrix[index, :] = word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            embedding_matrix[index, :] = word_to_vec_map['unk']\n",
    "    \n",
    "    # we don't want the embeddings to be updated, thus trainable parameter is set to False\n",
    "    decoder_embedding = Embedding(\n",
    "        input_dim=vocabulary_length,\n",
    "        output_dim=embedding_dimensions,\n",
    "        trainable=False,\n",
    "        name='decoder_embedding'\n",
    "    )\n",
    "    decoder_embedding.build((None,))\n",
    "    decoder_embedding.set_weights([embedding_matrix])  # with this the layer is now pretrained\n",
    "    \n",
    "    return decoder_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal state-sizes of LSTMs\n",
    "state_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer to receive the predictions from the feature model\n",
    "feature_input = Input(\n",
    "    shape=K.int_shape(feature_model.output)[1:], name='feature_input'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape feature transfer values\n",
    "image_model_output = Dense(state_size, activation='relu', name='image_model_output')(feature_input)\n",
    "image_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model to encode captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This inputs topic-transfer-values to the LSTM\n",
    "topic_input = Input(\n",
    "    shape=K.int_shape(topic_model.output)[1:], name='topic_input'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for token-sequences to the decoder\n",
    "# Using 'None' in the shape means that the token-sequences can have arbitrary lengths\n",
    "caption_input = Input(shape=(max_tokens,), name='caption_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding-layer which converts sequences of integer-tokens to sequences of vectors\n",
    "caption_embedding = create_embedding_layer(tokenizer.word_index, word_to_vec_map, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM layer for the input captions\n",
    "caption_lstm = LSTM(state_size, name='caption_lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the topic transfer values to 3D tensor in-order to feed it to the LSTM\n",
    "topic_input_reshaped = Reshape(\n",
    "    target_shape=(K.int_shape(topic_input)[1:] + (1,))\n",
    ")(topic_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed topic to LSTM\n",
    "_, initial_state_h0, initial_state_c0 = LSTM(\n",
    "    state_size, return_state=True, name='topic_lstm'\n",
    ")(topic_input_reshaped)\n",
    "\n",
    "topic_lstm_states = [initial_state_h0, initial_state_c0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = caption_input  # Start the decoder-network with its input-layer\n",
    "net = caption_embedding(net)  # Connect the embedding-layer\n",
    "caption_model_output = caption_lstm(net, initial_state=topic_lstm_states) # Connect the caption LSTM layer\n",
    "caption_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the image and the caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the outputs of both the models\n",
    "merge_net = Add()([image_model_output, caption_model_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_net = Dense(state_size, activation='relu')(merge_net)\n",
    "outputs = Dense(vocab_size, activation='softmax', name='caption_output')(merge_net)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[feature_input, topic_input, caption_input],\n",
    "    outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'weights/checkpoint1.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./weights/logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_checkpoint, callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "    print('Weights loaded.')\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='caption_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit_generator(\n",
    "    generator=generator_train,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(topic_values, feature_values, caption_model, tokenizer, max_tokens):\n",
    "    # Start with the initial start token\n",
    "    predicted_caption = 'startseq'\n",
    "    \n",
    "    # Input for the caption model\n",
    "    x_data = {\n",
    "        'topic_input': np.expand_dims(topic_values, axis=0),\n",
    "        'feature_input': np.expand_dims(feature_values, axis=0)\n",
    "    }\n",
    "    \n",
    "    for i in range(max_tokens):\n",
    "        sequence = tokenizer.texts_to_sequences([predicted_caption])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_tokens)\n",
    "        \n",
    "        # predict next word\n",
    "        x_data['caption_input'] = np.array(sequence)\n",
    "        y_pred = caption_model.predict(x_data, verbose=0)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        word = tokenizer.index_word[y_pred]\n",
    "        \n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        # append as input for generating the next word\n",
    "        predicted_caption += ' ' + word\n",
    "        \n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    \n",
    "    return ' '.join(predicted_caption.split()[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "image_path = 'dataset/' + images_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image.\n",
    "plt.imshow(load_image(image_path))\n",
    "plt.show()\n",
    "\n",
    "# Predicted Caption\n",
    "predicted_caption = generate_captions(\n",
    "    topic_transfer_values_test[idx], feature_transfer_values_test[idx], model, tokenizer, max_tokens\n",
    ")\n",
    "print('Predicted Caption:')\n",
    "print(predicted_caption)\n",
    "\n",
    "# Original Captions\n",
    "print('Original Captions:')\n",
    "print(captions_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(topic_values, feature_values, caption_model, captions, tokenizer, max_tokens):\n",
    "    actual, predicted = [], []\n",
    "    captions_length = len(captions)\n",
    "    \n",
    "    # Initial call to print 0% progress\n",
    "    print_progress_bar_counter = 0\n",
    "    print_progress_bar(print_progress_bar_counter, captions_length)\n",
    "    \n",
    "    for idx in range(captions_length):\n",
    "        # generate description\n",
    "        y_pred = generate_captions(\n",
    "            topic_values[idx], feature_values[idx], caption_model, tokenizer, max_tokens\n",
    "        )\n",
    "        \n",
    "        # store actual and predicted\n",
    "        references = [caption.split() for caption in captions[idx]]\n",
    "        actual.append(references)\n",
    "        predicted.append(y_pred.split())\n",
    "        \n",
    "        # Update Progress Bar\n",
    "        print_progress_bar_counter += 1\n",
    "        print_progress_bar(print_progress_bar_counter, captions_length)\n",
    "\n",
    "    print()\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate_model(\n",
    "    topic_transfer_values_val, feature_transfer_values_val, model, captions_val, tokenizer, max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
